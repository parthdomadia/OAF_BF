{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sort dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gpt_v2 import GPTLanguageModel\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\" # nvidia\n",
    "print(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13854, 5)\n",
      "samples: 0    \\r\\r\\nDog bone, stapler,\\r\\r\\ncribbage board, ...\n",
      "1    \\r\\r\\nThe old cupola glinted above the clouds,...\n",
      "2    \\r\\r\\nLook for me under the hood\\r\\r\\nof that ...\n",
      "3    \\r\\r\\nBehind the silo, the Mother Rabbit\\r\\r\\n...\n",
      "4    \\r\\r\\nWhen I push your button\\r\\r\\nyou fly off...\n",
      "Name: Poem, dtype: object\n",
      "Cleaned Sample Poems: [\"  \\nDog bone, stapler,  \\ncribbage board, garlic press  \\n     because this window is looseâ€”lacks  \\nsuction, lacks grip.  \\nBungee cord, bootstrap,  \\ndog leash, leather belt  \\n     because this window had sash cords.  \\nThey frayed. They broke.  \\nFeather duster, thatch of straw, empty  \\nbottle of Elmer's glue  \\n     because this window is loudâ€”its hinges clack  \\nopen, clack shut.  \\nStuffed bear, baby blanket,  \\nsingle crib newel  \\n     because this window is split. It's dividing  \\nin two.  \\nVelvet moss, sagebrush,  \\nwillow branch, robin's wing  \\n     because this window, it's pane-less. It's only  \\na frame of air.  \\n\"]\n"
     ]
    }
   ],
   "source": [
    "#code if we are using peotry based dataset\n",
    "file_path = \"C:\\\\Users\\\\parth\\\\code_master\\\\haiku.exe\\\\OAF_resources\\\\PoetryFoundationData.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.shape)\n",
    "\n",
    "texts = df['Poem']\n",
    "print(\"samples:\", texts[:5])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\r\", \" \")  # Remove carriage return\n",
    "    # text = text.replace(\"\\n\", \" \")  # Replace newlines with a space\n",
    "    # text = re.sub(r\"\\s+\", \" \", text)  # Normalize excessive spaces\n",
    "    # text = text.strip()  # Trim leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to each poem\n",
    "texts = [clean_text(poem) for poem in texts]\n",
    "\n",
    "# Display cleaned samples\n",
    "print(\"Cleaned Sample Poems:\", texts[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Vocabulary Size: 133144\n",
      "Tokenized data shape (should be 1D): torch.Size([3500703])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token=\"<OOV>\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\n\\t')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\" Vocabulary Size:\", vocab_size)\n",
    "\n",
    "# Flatten sequences into a **single continuous sequence**\n",
    "flattened_data = [token for seq in sequences for token in seq]  # 1D list\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "data = torch.tensor(flattened_data, dtype=torch.long)  # Ensure it's 1D\n",
    "print(\"Tokenized data shape (should be 1D):\", data.shape)\n",
    "\n",
    "# Split into training and validation sets\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]  # Training on 90% of the tokens\n",
    "val_data = data[n:]    # Validation on remaining 10%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split, batch_size, context_size):\n",
    "  data = train_data if split == 'train' else val_data\n",
    "  # print(f\"ðŸ›  Before batching, data shape: {data.shape}\")  \n",
    "\n",
    "  ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "  \n",
    "  # FIX: Make sure the shape is (batch_size, context_size)\n",
    "  x = torch.stack([data[i:i+context_size].clone().detach() for i in ix])  # Expected shape: (batch_size, context_size)\n",
    "  y = torch.stack([data[i+1:i+context_size+1].clone().detach() for i in ix])  # Expected shape: (batch_size, context_size)\n",
    "  \n",
    "  x, y = x.to(device), y.to(device)\n",
    "\n",
    "  # print(f\"âœ… get_batch - x shape: {x.shape}, y shape: {y.shape}\") \n",
    "  return x, y\n",
    "\n",
    "\n",
    "def generate_1(model, context_size, start_idx, number_of_tokens):\n",
    "  idx = start_idx\n",
    "  for _ in range(number_of_tokens):\n",
    "    # crop to last block_size of tokens\n",
    "    idx_cond = idx[:, -context_size:]\n",
    "    logits, loss = model(idx_cond)\n",
    "    # apply softmas to get probabilities\n",
    "    logits = logits[:, -1, :] # (batch_size, context_size)\n",
    "    probs = F.softmax(logits, dim=1) # (batch_size, context_size)\n",
    "    idx_next = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
    "    idx = torch.cat((idx, idx_next), dim=1) # (batch_size, t + 1)\n",
    "    # print(f\"From generate_1 function, idx shape: {idx.shape}\")\n",
    "  return idx\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, context_size, eval_iters=100):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size, context_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "def train(model, steps, batch_size, context_size, report_frequency=500, lr=1e-3):\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "  # print(f\"From train function, batch_size: {batch_size}, context_size: {context_size}\")\n",
    "  for step in range(steps): # increase number of steps for good results...\n",
    "      # sample a batch of data\n",
    "      xb, yb = get_batch('train', batch_size, context_size)\n",
    "\n",
    "      # evaluate the loss\n",
    "      logits, loss = model(xb, yb)\n",
    "      optimizer.zero_grad(set_to_none=True)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      if step % report_frequency == 0 or step == steps - 1:\n",
    "          losses = estimate_loss(model, batch_size, context_size)\n",
    "          print(f\"Step {step}, train loss: {losses['train']:.4f} val loss: {losses['val']:.4f}\")\n",
    "\n",
    "def decode_sequence(sequence, tokenizer):\n",
    "    reverse_word_index = {index: word for word, index in tokenizer.word_index.items()}\n",
    "    return \" \".join(reverse_word_index.get(token, \"<OOV>\") for token in sequence)\n",
    "\n",
    "def train_generate_print(model, steps=5000, batch_size=32, context_size=8, lr=1e-3):\n",
    "  # print(f\"From train_generate function, batch_size: {batch_size}, context_size: {context_size}\")\n",
    "  train(model, steps, batch_size, context_size, lr=lr)\n",
    "\n",
    "  start_idx = torch.zeros((1, 1),  dtype=torch.long, device=device)\n",
    "  max_tokens = 300\n",
    "  print(decode_sequence(\n",
    "      generate_1(model, context_size, start_idx=start_idx, number_of_tokens=max_tokens)[0].tolist()\n",
    "    )\n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, train loss: 11.2047 val loss: 11.1927\n",
      "Step 500, train loss: 7.0231 val loss: 7.0720\n",
      "Step 1000, train loss: 6.8112 val loss: 6.9577\n",
      "Step 1500, train loss: 6.5816 val loss: 6.8530\n",
      "Step 2000, train loss: 6.4776 val loss: 6.8426\n",
      "Step 2500, train loss: 6.3544 val loss: 6.7406\n",
      "Step 3000, train loss: 6.2257 val loss: 6.7543\n",
      "Step 3500, train loss: 6.1338 val loss: 6.7212\n",
      "Step 4000, train loss: 6.0238 val loss: 6.7894\n",
      "Step 4500, train loss: 5.9119 val loss: 6.7484\n",
      "Step 4999, train loss: 5.7932 val loss: 6.7989\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "decode_sequence() missing 1 required positional argument: 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 16\u001b[0m\n\u001b[0;32m      6\u001b[0m n_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[0;32m      9\u001b[0m m \u001b[38;5;241m=\u001b[39m GPTLanguageModel(\n\u001b[0;32m     10\u001b[0m   vocab_size,\n\u001b[0;32m     11\u001b[0m   n_embd\u001b[38;5;241m=\u001b[39mn_embd,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m   n_layer\u001b[38;5;241m=\u001b[39mn_layer\n\u001b[0;32m     15\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrain_generate_print\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mm\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1e6\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 72\u001b[0m, in \u001b[0;36mtrain_generate_print\u001b[1;34m(model, steps, batch_size, context_size, lr)\u001b[0m\n\u001b[0;32m     70\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),  dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     71\u001b[0m max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdecode_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerate_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: decode_sequence() missing 1 required positional argument: 'tokenizer'"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "context_size = 128\n",
    "lr = 3e-4\n",
    "n_embd = 384\n",
    "n_heads = 6\n",
    "n_layer = 6\n",
    "\n",
    "\n",
    "m = GPTLanguageModel(\n",
    "  vocab_size,\n",
    "  n_embd=n_embd,\n",
    "  context_size=context_size,\n",
    "  n_head=n_heads,\n",
    "  n_layer=n_layer\n",
    ").to(device)\n",
    "train_generate_print(m, batch_size=batch_size, context_size=context_size)\n",
    "\n",
    "print(f\"Total parameters: {sum(p.numel() for p in m.parameters()) / 1e6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and tokenizer saved!\n"
     ]
    }
   ],
   "source": [
    "# save the model and tokenizer:\n",
    "\n",
    "torch.save(m.state_dict(), 'trained_model_alt.pth')\n",
    "\n",
    "# Save tokenizer using pickle\n",
    "with open('tokenizer_alt.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"Model and tokenizer saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTLanguageModel(\n",
       "  (token_embedding_table): Embedding(133144, 384)\n",
       "  (position_embedding_table): Embedding(128, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=133144, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load trained model\n",
    "\n",
    "vocab_size = 133144\n",
    "batch_size = 16\n",
    "context_size = 128\n",
    "lr = 3e-4\n",
    "n_embd = 384\n",
    "n_heads = 6\n",
    "n_layer = 6\n",
    "\n",
    "model = GPTLanguageModel(vocab_size,n_embd,context_size,n_heads,n_layer)\n",
    "\n",
    "model.load_state_dict(torch.load('trained_model_alt.pth'))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer using pickle\n",
    "with open(\"tokenizer_alt.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequence(text, tokenizer):\n",
    "    return tokenizer.texts_to_sequences([text])[0]  # Convert text to sequence of token IDs\n",
    "\n",
    "def decode_sequence(seq, tokenizer):\n",
    "    index_to_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "    return ' '.join([index_to_word.get(i, \"<OOV>\") for i in seq])\n",
    "\n",
    "# Example prompt\n",
    "prompt_text = \"The endless void of \"\n",
    "start_tokens = encode_sequence(prompt_text, tokenizer)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "start_tokens = torch.tensor(start_tokens, dtype=torch.long).unsqueeze(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " the endless void of cold threshold leave the same that old narrative of its holiness originally you are always clearly and closer to a thrush or being a box sweeper as executioners who smells in the heart near waiting rooms the people sing cartilage\n"
     ]
    }
   ],
   "source": [
    "# Define how many tokens to generate\n",
    "num_tokens_to_generate = 40\n",
    "\n",
    "# Run inference (generate new tokens)\n",
    "with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "    generated_sequence = model.generate(start_tokens, num_tokens_to_generate)\n",
    "\n",
    "# Convert generated sequence back to text\n",
    "generated_text = decode_sequence(generated_sequence.squeeze(0).tolist(), tokenizer)\n",
    "\n",
    "print(\"Generated Text:\\n\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# format the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the endless void of cold threshold leave the\n",
      "same that old narrative of its holiness originally\n",
      "you are always clearly and closer to a\n",
      "thrush or being a box sweeper as executioners\n",
      "who smells in the heart near waiting rooms\n",
      "the people sing cartilage\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def format_poem_advanced(text, max_words_per_line=8):\n",
    "    # Ensure consistent spacing\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "    # Split based on punctuation for natural line breaks\n",
    "    sentences = re.split(r'([,.;â€”])', text)  # Keep punctuation in the split\n",
    "    processed_lines = []\n",
    "    current_line = \"\"\n",
    "\n",
    "    for part in sentences:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "\n",
    "        # If it's punctuation, append it to the current line\n",
    "        if part in \",.;â€”\":\n",
    "            current_line += part\n",
    "            continue\n",
    "\n",
    "        words = part.split()\n",
    "        while words:\n",
    "            if len(current_line.split()) + len(words) <= max_words_per_line:\n",
    "                current_line += (\" \" if current_line else \"\") + \" \".join(words)\n",
    "                words = []\n",
    "            else:\n",
    "                # Split if the line gets too long\n",
    "                processed_lines.append(current_line)\n",
    "                current_line = \" \".join(words[:max_words_per_line])\n",
    "                words = words[max_words_per_line:]\n",
    "\n",
    "        if current_line and current_line[-1] in \",.;â€”\":\n",
    "            processed_lines.append(current_line)\n",
    "            current_line = \"\"\n",
    "\n",
    "    # Add any remaining text\n",
    "    if current_line:\n",
    "        processed_lines.append(current_line)\n",
    "\n",
    "    return \"\\n\".join(processed_lines)\n",
    "\n",
    "\n",
    "formatted_poem = format_poem_advanced(generated_text)\n",
    "print(formatted_poem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
